{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf341b8-7bb7-4d19-a784-b5cf64504e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 8 \n",
    "def count_parameters_conv(in_channels: int, out_channels: int, kernel_size: int, bias: bool):\n",
    "    weight_params = in_channels * kernel_size**2 * out_channels\n",
    "        \n",
    "    if bias:\n",
    "        total_bias = out_channels\n",
    "        total_params = weight_params + total_bias\n",
    "    else:\n",
    "        total_params = weight_params\n",
    "    return total_params   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a15f2c7-1a25-4470-8a93-aa63414182c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73856"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = count_parameters_conv(in_channels=64, out_channels=128, kernel_size=3, bias=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de420560-deae-44d5-b893-bb43d4be4a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from IPython.display import clear_output\n",
    "from time import perf_counter\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0afe9e6e-7752-4193-8819-91a444ca77e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = MNIST(\n",
    "    \"../datasets/mnist\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=T.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3cabf54-a06d-4372-ab38-4715efdffb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_valid = MNIST(\n",
    "    \"../datasets/mnist\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=T.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f77694df-639c-4589-a23f-227117f11052",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(mnist_train, batch_size = 64, shuffle = True)\n",
    "valid_loader = DataLoader(mnist_valid, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3139a22e-858a-4713-bc9b-998b1a4b46e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 9\n",
    "def train(model: nn.Module, data_loader: DataLoader, optimizer: Optimizer, loss_fn) -> tuple[float, float]:\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    for x, y in tqdm(data_loader, desc='Train'):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(x)\n",
    "\n",
    "        loss = loss_fn(output, y)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "    train_loss = total_loss / len(data_loader)\n",
    "    return train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604cf575-1dd3-4f64-9e13-f989ed29986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "\n",
    "def evaluate(model: nn.Module, data_loader: DataLoader, loss_fn) -> tuple[float, float]:\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for x, y in tqdm(data_loader, desc='Evaluate'):\n",
    "        output = model(x)\n",
    "\n",
    "        loss = loss_fn(output, y)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, y_pred = torch.max(output, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (y_pred == y).sum().item()\n",
    "    valid_loss = total_loss / len(data_loader)\n",
    "    valid_accuracy = correct / total\n",
    "\n",
    "    return valid_loss, valid_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3788706f-a725-4fca-9e9a-b906152a6e00",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def create_mlp_model():\n",
    "    model = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(28 * 28, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 10)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "318e82a6-04a5-4bfd-982c-333676cb996d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_mlp_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m create_mlp_model()\n\u001b[1;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)  \u001b[38;5;66;03m# Добавляем оптимизатор\u001b[39;00m\n\u001b[1;32m      5\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss() \n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_mlp_model' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "\n",
    "model = create_mlp_model()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # Добавляем оптимизатор\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_model_weights = None\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, loss_fn)  \n",
    "    valid_loss, valid_accuracy = evaluate(model, valid_loader, loss_fn)\n",
    "    \n",
    "    # Проверяем, является ли текущая модель лучшей\n",
    "    if valid_accuracy > best_accuracy:\n",
    "        best_accuracy = valid_accuracy\n",
    "        best_epoch = epoch + 1\n",
    "        \n",
    "print(f'Эпоха: {best_epoch}')\n",
    "print(f'Точность на валидации: {best_accuracy:.5f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd715d53-03fc-466c-83d8-9fe8e1dd46d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d46629dc-36aa-462c-bc1b-5da4da9f87fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "def create_conv_model():\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(4 * 4 * 64, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 10)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c78fe13-09ac-4cc1-b03d-4dedee995c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv_model():\n",
    "    model = nn.Sequential(\n",
    "        # Первый блок - увеличено до 64 каналов (вдвое)\n",
    "        nn.Conv2d(in_channels=1, out_channels=64, kernel_size=5),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "        \n",
    "        # Второй блок - увеличиваем до 128 каналов (вдвое)\n",
    "        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "        \n",
    "        # Добавлен третий сверточный блок\n",
    "        nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        # Добавлен четвертый сверточный блок\n",
    "        nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "        nn.Flatten(),\n",
    "        # Обновлен размер входа для Linear (учитываем новые слои)\n",
    "        nn.Linear(2 * 2 * 512, 512),  # Увеличиваем до 512 нейронов\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 256),  # Добавлен дополнительный линейный слой\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 10)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f8f9465-b71d-4b30-9251-0ff53e9d277a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████| 938/938 [00:48<00:00, 19.54it/s]\n",
      "Evaluate: 100%|███████████████████████████████| 157/157 [00:02<00:00, 55.36it/s]\n",
      "Train: 100%|██████████████████████████████████| 938/938 [00:48<00:00, 19.48it/s]\n",
      "Evaluate: 100%|███████████████████████████████| 157/157 [00:02<00:00, 55.35it/s]\n",
      "Train: 100%|██████████████████████████████████| 938/938 [00:48<00:00, 19.49it/s]\n",
      "Evaluate: 100%|███████████████████████████████| 157/157 [00:02<00:00, 56.02it/s]\n",
      "Train: 100%|██████████████████████████████████| 938/938 [00:47<00:00, 19.62it/s]\n",
      "Evaluate: 100%|███████████████████████████████| 157/157 [00:02<00:00, 54.69it/s]\n",
      "Train: 100%|██████████████████████████████████| 938/938 [00:48<00:00, 19.20it/s]\n",
      "Evaluate: 100%|███████████████████████████████| 157/157 [00:02<00:00, 54.28it/s]\n",
      "Train: 100%|██████████████████████████████████| 938/938 [00:47<00:00, 19.58it/s]\n",
      "Evaluate: 100%|███████████████████████████████| 157/157 [00:02<00:00, 54.12it/s]\n",
      "Train: 100%|██████████████████████████████████| 938/938 [00:47<00:00, 19.64it/s]\n",
      "Evaluate: 100%|███████████████████████████████| 157/157 [00:02<00:00, 53.32it/s]\n",
      "Train: 100%|██████████████████████████████████| 938/938 [00:47<00:00, 19.60it/s]\n",
      "Evaluate: 100%|███████████████████████████████| 157/157 [00:02<00:00, 52.54it/s]\n",
      "Train: 100%|██████████████████████████████████| 938/938 [00:48<00:00, 19.22it/s]\n",
      "Evaluate: 100%|███████████████████████████████| 157/157 [00:03<00:00, 43.42it/s]\n",
      "Train: 100%|██████████████████████████████████| 938/938 [00:50<00:00, 18.58it/s]\n",
      "Evaluate: 100%|███████████████████████████████| 157/157 [00:03<00:00, 42.96it/s]\n",
      "Train: 100%|██████████████████████████████████| 938/938 [00:51<00:00, 18.35it/s]\n",
      "Evaluate: 100%|███████████████████████████████| 157/157 [00:03<00:00, 43.01it/s]\n",
      "Train: 100%|██████████████████████████████████| 938/938 [00:51<00:00, 18.19it/s]\n",
      "Evaluate: 100%|███████████████████████████████| 157/157 [00:03<00:00, 42.45it/s]\n",
      "Train: 100%|██████████████████████████████████| 938/938 [00:51<00:00, 18.20it/s]\n",
      "Evaluate: 100%|███████████████████████████████| 157/157 [00:03<00:00, 42.52it/s]\n",
      "Train: 100%|██████████████████████████████████| 938/938 [00:52<00:00, 17.92it/s]\n",
      "Evaluate: 100%|███████████████████████████████| 157/157 [00:03<00:00, 41.97it/s]\n",
      "Train: 100%|██████████████████████████████████| 938/938 [00:52<00:00, 17.94it/s]\n",
      "Evaluate: 100%|███████████████████████████████| 157/157 [00:03<00:00, 41.57it/s]\n",
      "Train: 100%|██████████████████████████████████| 938/938 [00:52<00:00, 17.94it/s]\n",
      "Evaluate: 100%|███████████████████████████████| 157/157 [00:03<00:00, 41.30it/s]\n",
      "Train: 100%|██████████████████████████████████| 938/938 [00:53<00:00, 17.61it/s]\n",
      "Evaluate: 100%|███████████████████████████████| 157/157 [00:03<00:00, 41.15it/s]\n",
      "Train: 100%|██████████████████████████████████| 938/938 [00:52<00:00, 17.87it/s]\n",
      "Evaluate: 100%|███████████████████████████████| 157/157 [00:03<00:00, 41.10it/s]\n",
      "Train: 100%|██████████████████████████████████| 938/938 [00:52<00:00, 17.90it/s]\n",
      "Evaluate: 100%|███████████████████████████████| 157/157 [00:03<00:00, 40.00it/s]\n",
      "Train: 100%|██████████████████████████████████| 938/938 [00:52<00:00, 17.76it/s]\n",
      "Evaluate: 100%|███████████████████████████████| 157/157 [00:03<00:00, 40.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха: 14\n",
      "Точность на валидации: 0.9937%\n",
      "Время обученния 1073.99442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "model = create_conv_model()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # Добавляем оптимизатор\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_model_weights = None\n",
    "best_epoch = 0\n",
    "start = perf_counter()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, loss_fn)  \n",
    "    valid_loss, valid_accuracy = evaluate(model, valid_loader, loss_fn)\n",
    "    \n",
    "    # Проверяем, является ли текущая модель лучшей\n",
    "    if valid_accuracy > best_accuracy:\n",
    "        best_accuracy = valid_accuracy\n",
    "        best_epoch = epoch + 1\n",
    "        best_model_weights = model.state_dict().copy()\n",
    "        \n",
    "print(f'Эпоха: {best_epoch}')\n",
    "print(f'Точность на валидации: {best_accuracy:.4f}%')\n",
    "print(f'Время обученния {perf_counter() - start:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96d01e3c-97b5-4847-b8e0-230dbbf4370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model_weights, 'model_weights_conv10.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
